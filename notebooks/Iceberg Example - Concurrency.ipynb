{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61515c3b",
   "metadata": {},
   "source": [
    "### Iceberg Example - Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac161cf8",
   "metadata": {},
   "source": [
    "This is a simple example showing how Apache Iceberg has partially better concurrency than a PostgresDB. It shows how the \"simplicity\" of the Iceberg interface makes working with it pretty easy and fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b2ca205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.9/site-packages (2.9.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: faker in /usr/local/lib/python3.9/site-packages (13.11.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "### Basic Dependency stuff.\n",
    "import sys\n",
    "!pip install --prefix {sys.prefix} psycopg2 pandas faker\n",
    "\n",
    "import psycopg2\n",
    "from threading import Thread\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543e937",
   "metadata": {},
   "source": [
    "### The PostgresDB Version of Insert & Alter at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21863348",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining a few SQL Statements, \n",
    "drop_if_exists = \"\"\"DROP TABLE  IF EXISTS public.employee\"\"\"\n",
    "\n",
    "employee_ddl = \"\"\"CREATE TABLE public.employee (\n",
    "    id int8 NOT NULL,\n",
    "    name varchar(120) NOT NULL,\n",
    "    salary int8 NOT NULL,\n",
    "    CONSTRAINT emp_pk PRIMARY KEY (id)\n",
    ");\"\"\"\n",
    "\n",
    "sql_dummy_data = \"\"\"WITH salary_list AS (\n",
    "    SELECT '{1000, 2000, 5000}'::INT[] salary\n",
    ")\n",
    "INSERT INTO public.employee\n",
    "(id, name, salary)\n",
    "SELECT n, 'Employee ' || n as name, salary[1 + mod(n, array_length(salary, 1))]\n",
    "FROM salary_list, generate_series(1, 1000000) as n\"\"\"\n",
    "\n",
    "# Just in case you're wondering, on my machine the insert takes: 33s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1f2bf",
   "metadata": {},
   "source": [
    "**Task**: We going to insert a million rows of data into our table, and while we're doing this, we're also renaming a couple of columns. In parallel, because stuff happens in parallel, and if we just chain stuff up, well, the day only has 24 hours, right?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bbf0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Task 2 about to add column 1\n",
      "Task 2 finished adding a column\n",
      "Task 2 about to add column 2\n",
      "Task 2 finished adding a column\n",
      "Task 2 about to add column 3\n",
      "Task 2 finished adding a column\n",
      "Task 2 about to add column 4\n",
      "Task 2 finished adding a column\n",
      "Task 2 about to add column 5\n",
      "Task 2 finished adding a column\n",
      "starting task 1\n",
      "Task 4 starts to rename stuff\n",
      "Task 4 about to rename column 1\n",
      "finished task 1\n",
      "Task 4 finished adding a column\n",
      "Task 4 about to rename column 2\n",
      "Task 4 finished adding a column\n",
      "Task 4 about to rename column 3\n",
      "Task 4 finished adding a column\n",
      "Task 4 about to rename column 4\n",
      "Task 4 finished adding a column\n",
      "Task 4 about to rename column 5\n",
      "Task 4 finished adding a column\n",
      "Task 4 finished\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(host=\"postgres\", port = 5432, database=\"demo_catalog\", user=\"admin\", password=\"password\")\n",
    "cur = conn.cursor()\n",
    "  \n",
    "############--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#-----\n",
    "## Delete possible old table, create the new one, add a bunch of columns, get ready!\n",
    "\n",
    "cur.execute(drop_if_exists)\n",
    "cur.execute(employee_ddl)\n",
    "cur.execute(f\"\"\"SELECT * FROM public.employee\"\"\")\n",
    "print(cur.fetchall())\n",
    "\n",
    "# Add columns.\n",
    "\n",
    "count = 0\n",
    "while count < 5:\n",
    "    count += 1\n",
    "    cur.execute(f\"\"\"ALTER TABLE public.employee ADD COLUMN last_column_{count} timestamptz;\"\"\") \n",
    "    print(\"Adding a column\")\n",
    "\n",
    "time.sleep(5)\n",
    "############--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#-----\n",
    "## Define our two concurrent tasks\n",
    "    \n",
    "def task1( threadName, delay):\n",
    "    print(\"starting task 1; Inserting data\")\n",
    "    cur.execute(sql_dummy_data)\n",
    "    print(\"finished task 1; Inserting data\")\n",
    "    \n",
    "def task4( threadName, delay):\n",
    "    count=0\n",
    "    print(f\"Task 4 starts to rename stuff\")\n",
    "    while count < 5:\n",
    "        print(f\"Task 4 about to rename column {count+1}\")\n",
    "        count += 1\n",
    "        cur.execute(f\"\"\"ALTER TABLE public.employee RENAME last_column_{count} TO new_column_{count} \"\"\") \n",
    "        print(\"Task 4 finished adding a column\")\n",
    "\n",
    "    print(\"Task 4 finished\")\n",
    "\n",
    "\n",
    "############--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#-----\n",
    "## Let them run...\n",
    "\n",
    "# create two new threads\n",
    "t1 = Thread(target=task1,args=(\"Thread-1\", 2))\n",
    "t4 = Thread(target=task4,args=(\"Thread-4\", 2))\n",
    "\n",
    "t1.start()\n",
    "t4.start()\n",
    "\n",
    "# wait for the threads to complete\n",
    "t1.join()\n",
    "t4.join()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af58bba",
   "metadata": {},
   "source": [
    "### The same two tasks running through Apache Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad878f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-iceberg:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd6332fb970>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d699b779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS nyc.employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d45692c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmissing '(' at 'salary'(line 5, pos 17)\n\n== SQL ==\nCREATE TABLE nyc.employee (\n    id int,\n    name varchar(120),\n    salary int\n) PARTITIONED BY salary;\n-----------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m employee_ddl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCREATE TABLE nyc.employee (\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    id int,\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    name varchar(120),\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    salary int\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m) PARTITIONED BY salary;\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43memployee_ddl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmissing '(' at 'salary'(line 5, pos 17)\n\n== SQL ==\nCREATE TABLE nyc.employee (\n    id int,\n    name varchar(120),\n    salary int\n) PARTITIONED BY salary;\n-----------------^^^\n"
     ]
    }
   ],
   "source": [
    "employee_ddl = \"\"\"CREATE TABLE nyc.employee (\n",
    "    id int,\n",
    "    name varchar(120),\n",
    "    salary int;\"\"\"\n",
    "\n",
    "spark.sql(employee_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e0aed31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO nyc.employee\n",
    "(id, name, salary)\n",
    "VALUES (1, 'bob', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cff2f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|id |name|salary|\n",
      "+---+----+------+\n",
      "|1  |bob |1000  |\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"nyc.employee\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97846800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000001\n",
      "am here\n",
      "starting task 1 on Iceberg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 12:26:46 WARN TaskSetManager: Stage 57 contains a task of very large size (52559 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/16 12:26:47 WARN TaskSetManager: Stage 58 contains a task of very large size (52559 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/16 12:26:48 WARN TaskSetManager: Stage 59 contains a task of very large size (52559 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 59:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start task 2 for renaming things..\n",
      "finish renaming things...\n",
      "start task 2 now inserting one data piece..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 2 done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished task 1 on Iceberg\n",
      "+------+--------+----------+\n",
      "|new_id|new_name|new_salary|\n",
      "+------+--------+----------+\n",
      "|1     |bob     |1000      |\n",
      "|1     |Bob     |1200      |\n",
      "|2     |Bob0    |0         |\n",
      "|2     |Bob1    |1         |\n",
      "|2     |Bob2    |2         |\n",
      "|2     |Bob3    |3         |\n",
      "|2     |Bob4    |4         |\n",
      "|2     |Bob5    |5         |\n",
      "|2     |Bob6    |6         |\n",
      "|2     |Bob7    |7         |\n",
      "|2     |Bob8    |8         |\n",
      "|2     |Bob9    |9         |\n",
      "|2     |Bob10   |10        |\n",
      "|2     |Bob11   |11        |\n",
      "|2     |Bob12   |12        |\n",
      "|2     |Bob13   |13        |\n",
      "|2     |Bob14   |14        |\n",
      "|2     |Bob15   |15        |\n",
      "|2     |Bob16   |16        |\n",
      "|2     |Bob17   |17        |\n",
      "+------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#-----\n",
    "## Some preparation data & SQLs\n",
    "\n",
    "data = [(1, \"Bob\",1200)]\n",
    "for i in range(0,10000000):\n",
    "    data.append((2,f\"Bob{i}\",i))\n",
    "print(f\"Created {len(data)} data rows to insert\")\n",
    "\n",
    "# if you're wondering why there's 10 million rows here and just 1 million before: On my machine it roughly is 10 \n",
    "# times faster to write to my local file system through Iceberg than through the Postgres\n",
    "\n",
    "single_data = [(1, \"Bob\",1200)]\n",
    "\n",
    "alter_sql_1 = \"\"\"ALTER TABLE nyc.employee RENAME COLUMN salary TO new_salary;\"\"\"\n",
    "alter_sql_2 = \"\"\"ALTER TABLE nyc.employee RENAME COLUMN id TO new_id;\"\"\"\n",
    "alter_sql_3 = \"\"\"ALTER TABLE nyc.employee RENAME COLUMN name TO new_name;\"\"\"\n",
    "\n",
    "############--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#-----\n",
    "## Define our two concurrent tasks\n",
    "\n",
    "\n",
    "def task1():\n",
    "    print(\"starting task 1 on Iceberg; Inserting data\")\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    df = rdd.toDF()\n",
    "    columns = [\"id\",\"name\",\"salary\"]\n",
    "    df = rdd.toDF(columns)\n",
    "    df.write.mode(\"append\").saveAsTable(\"nyc.employee\")\n",
    "    # write takes about 25 secs and is 10 times the size of the other write... \n",
    "\n",
    "    print(\"finished task 1 on Iceberg; Inserting data\")\n",
    "\n",
    "def task2():\n",
    "    time.sleep(10)\n",
    "    print(\"start task 2 for renaming things..\")\n",
    "    spark.sql(alter_sql_1)\n",
    "    spark.sql(alter_sql_2)\n",
    "    spark.sql(alter_sql_3)\n",
    "    \n",
    "    print(\"finish renaming things...\")\n",
    "   \n",
    "    print(\"start task 2 now inserting one data piece..\")\n",
    "\n",
    "    rdd = spark.sparkContext.parallelize(single_data)\n",
    "    df = rdd.toDF()\n",
    "    columns = [\"new_id\",\"new_name\",\"new_salary\"]\n",
    "    df = rdd.toDF(columns)\n",
    "    df.write.mode(\"append\").saveAsTable(\"nyc.employee\")\n",
    "    print(\"task 2 done..\")\n",
    "\n",
    "    #spark.read.format(\"iceberg\").load(\"nyc.employee\").show(truncate = False)\n",
    "\n",
    "############--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#--------#-----\n",
    "## Letting them run..\n",
    "\n",
    "# create two new threads\n",
    "t1 = Thread(target=task1)\n",
    "t2 = Thread(target=task2)\n",
    "\n",
    "# start the threads\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait for the threads to complete\n",
    "t1.join()\n",
    "t2.join()\n",
    "spark.read.format(\"iceberg\").load(\"nyc.employee\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab386d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>made_current_at</th>\n",
       "            <th>snapshot_id</th>\n",
       "            <th>parent_id</th>\n",
       "            <th>is_current_ancestor</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2022-05-16 12:24:42.446000</td>\n",
       "            <td>5645340098941460626</td>\n",
       "            <td>None</td>\n",
       "            <td>True</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2022-05-16 12:27:10.753000</td>\n",
       "            <td>4734883099060616137</td>\n",
       "            <td>5645340098941460626</td>\n",
       "            <td>True</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2022-05-16 12:27:11.660000</td>\n",
       "            <td>4648749835832900692</td>\n",
       "            <td>4734883099060616137</td>\n",
       "            <td>True</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------------------------+---------------------+---------------------+---------------------+\n",
       "|            made_current_at |         snapshot_id |           parent_id | is_current_ancestor |\n",
       "+----------------------------+---------------------+---------------------+---------------------+\n",
       "| 2022-05-16 12:24:42.446000 | 5645340098941460626 |                None |                True |\n",
       "| 2022-05-16 12:27:10.753000 | 4734883099060616137 | 5645340098941460626 |                True |\n",
       "| 2022-05-16 12:27:11.660000 | 4648749835832900692 | 4734883099060616137 |                True |\n",
       "+----------------------------+---------------------+---------------------+---------------------+"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM nyc.employee.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2cc5967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>committed_at</th>\n",
       "            <th>snapshot_id</th>\n",
       "            <th>parent_id</th>\n",
       "            <th>operation</th>\n",
       "            <th>manifest_list</th>\n",
       "            <th>summary</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2022-05-16 12:24:42.446000</td>\n",
       "            <td>5645340098941460626</td>\n",
       "            <td>None</td>\n",
       "            <td>append</td>\n",
       "            <td>/home/iceberg/warehouse/nyc/employee/metadata/snap-5645340098941460626-1-2fb68011-dbbf-4cd7-a1c6-89663f3fd5c5.avro</td>\n",
       "            <td>{&#x27;spark.app.id&#x27;: &#x27;local-1652690273198&#x27;, &#x27;changed-partition-count&#x27;: &#x27;1&#x27;, &#x27;added-data-files&#x27;: &#x27;1&#x27;, &#x27;total-equality-deletes&#x27;: &#x27;0&#x27;, &#x27;added-records&#x27;: &#x27;1&#x27;, &#x27;total-position-deletes&#x27;: &#x27;0&#x27;, &#x27;added-files-size&#x27;: &#x27;866&#x27;, &#x27;total-delete-files&#x27;: &#x27;0&#x27;, &#x27;total-files-size&#x27;: &#x27;866&#x27;, &#x27;total-records&#x27;: &#x27;1&#x27;, &#x27;total-data-files&#x27;: &#x27;1&#x27;}</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2022-05-16 12:27:10.753000</td>\n",
       "            <td>4734883099060616137</td>\n",
       "            <td>5645340098941460626</td>\n",
       "            <td>append</td>\n",
       "            <td>/home/iceberg/warehouse/nyc/employee/metadata/snap-4734883099060616137-1-1ac1affb-9e70-4a76-9687-48b047c70ee7.avro</td>\n",
       "            <td>{&#x27;spark.app.id&#x27;: &#x27;local-1652690273198&#x27;, &#x27;changed-partition-count&#x27;: &#x27;1&#x27;, &#x27;added-data-files&#x27;: &#x27;1&#x27;, &#x27;total-equality-deletes&#x27;: &#x27;0&#x27;, &#x27;added-records&#x27;: &#x27;1&#x27;, &#x27;total-position-deletes&#x27;: &#x27;0&#x27;, &#x27;added-files-size&#x27;: &#x27;922&#x27;, &#x27;total-delete-files&#x27;: &#x27;0&#x27;, &#x27;total-files-size&#x27;: &#x27;1788&#x27;, &#x27;total-records&#x27;: &#x27;2&#x27;, &#x27;total-data-files&#x27;: &#x27;2&#x27;}</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2022-05-16 12:27:11.660000</td>\n",
       "            <td>4648749835832900692</td>\n",
       "            <td>4734883099060616137</td>\n",
       "            <td>append</td>\n",
       "            <td>/home/iceberg/warehouse/nyc/employee/metadata/snap-4648749835832900692-1-bdabe93b-a772-44fc-b14e-cd89191b45be.avro</td>\n",
       "            <td>{&#x27;spark.app.id&#x27;: &#x27;local-1652690273198&#x27;, &#x27;changed-partition-count&#x27;: &#x27;1&#x27;, &#x27;added-data-files&#x27;: &#x27;4&#x27;, &#x27;total-equality-deletes&#x27;: &#x27;0&#x27;, &#x27;added-records&#x27;: &#x27;10000001&#x27;, &#x27;total-position-deletes&#x27;: &#x27;0&#x27;, &#x27;added-files-size&#x27;: &#x27;37326423&#x27;, &#x27;total-delete-files&#x27;: &#x27;0&#x27;, &#x27;total-files-size&#x27;: &#x27;37328211&#x27;, &#x27;total-records&#x27;: &#x27;10000003&#x27;, &#x27;total-data-files&#x27;: &#x27;6&#x27;}</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------------------------+---------------------+---------------------+-----------+--------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
       "|               committed_at |         snapshot_id |           parent_id | operation |                                                                                                      manifest_list |                                                                                                                                                                                                                                                                                                                                      summary |\n",
       "+----------------------------+---------------------+---------------------+-----------+--------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
       "| 2022-05-16 12:24:42.446000 | 5645340098941460626 |                None |    append | /home/iceberg/warehouse/nyc/employee/metadata/snap-5645340098941460626-1-2fb68011-dbbf-4cd7-a1c6-89663f3fd5c5.avro |                         {'spark.app.id': 'local-1652690273198', 'changed-partition-count': '1', 'added-data-files': '1', 'total-equality-deletes': '0', 'added-records': '1', 'total-position-deletes': '0', 'added-files-size': '866', 'total-delete-files': '0', 'total-files-size': '866', 'total-records': '1', 'total-data-files': '1'} |\n",
       "| 2022-05-16 12:27:10.753000 | 4734883099060616137 | 5645340098941460626 |    append | /home/iceberg/warehouse/nyc/employee/metadata/snap-4734883099060616137-1-1ac1affb-9e70-4a76-9687-48b047c70ee7.avro |                        {'spark.app.id': 'local-1652690273198', 'changed-partition-count': '1', 'added-data-files': '1', 'total-equality-deletes': '0', 'added-records': '1', 'total-position-deletes': '0', 'added-files-size': '922', 'total-delete-files': '0', 'total-files-size': '1788', 'total-records': '2', 'total-data-files': '2'} |\n",
       "| 2022-05-16 12:27:11.660000 | 4648749835832900692 | 4734883099060616137 |    append | /home/iceberg/warehouse/nyc/employee/metadata/snap-4648749835832900692-1-bdabe93b-a772-44fc-b14e-cd89191b45be.avro | {'spark.app.id': 'local-1652690273198', 'changed-partition-count': '1', 'added-data-files': '4', 'total-equality-deletes': '0', 'added-records': '10000001', 'total-position-deletes': '0', 'added-files-size': '37326423', 'total-delete-files': '0', 'total-files-size': '37328211', 'total-records': '10000003', 'total-data-files': '6'} |\n",
       "+----------------------------+---------------------+---------------------+-----------+--------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM nyc.employee.snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88b59867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|new_id|new_name|new_salary|\n",
      "+------+--------+----------+\n",
      "|1     |bob     |1000      |\n",
      "|1     |Bob     |1200      |\n",
      "|2     |Bob0    |0         |\n",
      "|2     |Bob1    |1         |\n",
      "|2     |Bob2    |2         |\n",
      "|2     |Bob3    |3         |\n",
      "|2     |Bob4    |4         |\n",
      "|2     |Bob5    |5         |\n",
      "|2     |Bob6    |6         |\n",
      "|2     |Bob7    |7         |\n",
      "|2     |Bob8    |8         |\n",
      "|2     |Bob9    |9         |\n",
      "|2     |Bob10   |10        |\n",
      "|2     |Bob11   |11        |\n",
      "|2     |Bob12   |12        |\n",
      "|2     |Bob13   |13        |\n",
      "|2     |Bob14   |14        |\n",
      "|2     |Bob15   |15        |\n",
      "|2     |Bob16   |16        |\n",
      "|2     |Bob17   |17        |\n",
      "+------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"snapshot-id\", 4648749835832900692).format(\"iceberg\").load(\"nyc.employee\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb6c0d",
   "metadata": {},
   "source": [
    "**Cool things about Iceberg**: \n",
    "1. The renaming is super fast (because it's just metadata). Renaming is also just a metadata operation in Postgres (correct me if I am wrong!), but changing data types would be just as fast with Iceberg, whereas it would take much longer on a PostgresDB. \n",
    "2. It works concurrently! Which doesn't work in Postgres (and creates a mess using just Hive tables).  \n",
    "\n",
    "Note: Yes of course you can use PostgresQL and tell it to not rewrite the table... \n",
    "\n",
    "Also note: Iceberg creates snapshots only on addition of new data! So if we would not insert a new data piece with the new columns, we wouldn't get a snapshot for the \"inbetween state\", which simply is nice to show off.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
